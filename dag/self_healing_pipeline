from airflow import DAG
from airflow.providers.google.cloud.operators.dataproc import DataprocSubmitJobOperator, DataprocCreateClusterOperator, DataprocDeleteClusterOperator
from airflow.operators.python_operator import PythonOperator
from airflow.utils.trigger_rule import TriggerRule
from airflow.utils.dates import days_ago
import sys
import os

sys.path.append(os.path.dirname(__file__))

from utils.vertex_ai_handler import analyze_error, suggest_fix
from utils.auto_healer import apply_fix
from utils.data_quality import run_data_quality_check

PROJECT_ID = "beaming-force-480014-v3"
REGION = "us-central1"
CLUSTER_NAME = "self-healing-cluster"
BUCKET_NAME = "gagansamplebucket"

default_args = {
    'owner': 'airflow',
    'start_date': days_ago(1),
    'retries': 0
}

def self_healing_callback(context):
    """Fetches Dataproc logs and applies AI-powered fixes."""
    from google.cloud import dataproc_v1, storage
    import re
    
    exception = str(context.get('exception'))
    print(f"Task failed: {exception[:200]}")
    
    error_log = ""
    
    try:
        job_id_match = re.search(r'job_id["\s:=]+([a-f0-9\-]+)', exception, re.IGNORECASE)
        
        if job_id_match:
            job_id = job_id_match.group(1)
            print(f"Dataproc job ID: {job_id}")
            
            job_client = dataproc_v1.JobControllerClient(
                client_options={"api_endpoint": f"{REGION}-dataproc.googleapis.com:443"}
            )
            
            job = job_client.get_job(project_id=PROJECT_ID, region=REGION, job_id=job_id)
            driver_output_uri = job.driver_output_resource_uri
            
            if driver_output_uri:
                gcs_path = driver_output_uri.replace('gs://', '')
                bucket_name, base_path = gcs_path.split('/', 1)
                
                storage_client = storage.Client(project=PROJECT_ID)
                bucket = storage_client.bucket(bucket_name)
                
                log_files = [base_path, base_path + '.000000000']
                
                for log_file in log_files:
                    try:
                        blob = bucket.blob(log_file)
                        log_content = blob.download_as_text()
                        
                        traceback_start = log_content.find("Traceback (most recent call last):")
                        if traceback_start != -1:
                            error_log = log_content[traceback_start:traceback_start + 4000]
                            print(f"Extracted Traceback ({len(error_log)} chars)")
                        else:
                            error_log = f"START:\n{log_content[:2000]}\n\nEND:\n{log_content[-2000:]}"
                        break
                    except:
                        continue
            else:
                error_log = f"Job Status: {job.status.state.name}\nDetails: {job.status.details}"
        else:
            error_log = exception
    except Exception as e:
        print(f"Log fetch error: {e}")
        error_log = exception
    
    print("\n=== SELF-HEALING ===")
    analysis = analyze_error(error_log, PROJECT_ID)
    fix = suggest_fix(analysis)
    
    print(f"Root Cause: {fix.get('root_cause')}")
    print(f"Fix Type: {fix.get('fix_type')}")
    print(f"Suggested Fix: {fix.get('suggested_fix')}")
    
    apply_fix(fix, __file__, f"gs://{BUCKET_NAME}/scripts/transform_script.py")

CLUSTER_CONFIG = {
    "master_config": {
        "num_instances": 1,
        "machine_type_uri": "n1-standard-2",
        "disk_config": {"boot_disk_type": "pd-standard", "boot_disk_size_gb": 30},
    },
    "worker_config": {
        "num_instances": 2,
        "machine_type_uri": "n1-standard-2",
        "disk_config": {"boot_disk_type": "pd-standard", "boot_disk_size_gb": 30},
    }
}

with DAG(
    'self_healing_pipeline',
    default_args=default_args,
    description='Self-healing data pipeline with AI error analysis',
    schedule_interval=None,
    catchup=False,
    params={
        "input_table": "employee_data",
        "output_table": "employee_data_output"
    }
) as dag:

    create_cluster = DataprocCreateClusterOperator(
        task_id="create_cluster",
        project_id=PROJECT_ID,
        cluster_config=CLUSTER_CONFIG,
        region=REGION,
        cluster_name=CLUSTER_NAME,
    )

    pyspark_job = {
        "reference": {"project_id": PROJECT_ID},
        "placement": {"cluster_name": CLUSTER_NAME},
        "pyspark_job": {
            "main_python_file_uri": f"gs://{BUCKET_NAME}/scripts/transform_script.py",
            "args": [
                "--project_id", PROJECT_ID,
                "--input_table", "{{ params.input_table }}",
                "--output_table", "{{ params.output_table }}"
            ],
        },
    }

    submit_spark_job = DataprocSubmitJobOperator(
        task_id="submit_spark_job",
        job=pyspark_job,
        region=REGION,
        project_id=PROJECT_ID,
        on_failure_callback=self_healing_callback
    )

    delete_cluster = DataprocDeleteClusterOperator(
        task_id="delete_cluster",
        project_id=PROJECT_ID,
        cluster_name=CLUSTER_NAME,
        region=REGION,
        trigger_rule=TriggerRule.ALL_DONE
    )

    quality_check = PythonOperator(
        task_id="data_quality_check",
        python_callable=run_data_quality_check,
        op_kwargs={
            'project_id': PROJECT_ID,
            'output_table': "{{ params.output_table }}"
        },
        provide_context=True
    )

    create_cluster >> submit_spark_job >> delete_cluster
    submit_spark_job >> quality_check
